<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SWattention: Designing Fast and Memory-Efficient Attention for a New Sunway Supercomputer</title>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>
        body {
            font-family: 'Times New Roman', Times, serif;
            line-height: 1.6;
        }
        p {
            text-align: justify;
        }
        a {
            color: #0066cc;
        }
        .citation {
            font-style: italic;
        }
    </style>
</head>
<body>

    <h1>SWattention: Designing Fast and Memory-Efficient Attention for a New Sunway Supercomputer</h1>
    <p class="citation">Ruohan Wu, Xianyu Zhu, Junshi Chen, Sha Liu, Tianyu Zheng, Xin Liu, Hong An, "SWattention: Designing Fast and Memory-Efficient Attention for a New Sunway Supercomputer." <em>The Journal of Supercomputing</em>, 2024.</p>

    <h2>Abstract</h2>
    <p>
        In the past few years, Transformer-based large language models (LLM) have become the dominant technology in a series of applications. To scale up the sequence length of the Transformer, FlashAttention is proposed to compute exact attention with reduced memory requirements and faster execution. However, implementing the FlashAttention algorithm on the new generation Sunway Supercomputer faces many constraints such as the unique heterogeneous architecture and the limited memory bandwidth.
    </p>
    <p>
        This work proposes SWattention, a highly efficient method for computing the exact attention on the SW26010pro processor. To fully utilize the 6 core groups (CG) and 64 cores per CG on the processor, we design a two-level parallel task partition strategy. Asynchronous memory access is employed to ensure that memory access overlaps with computation. Additionally, a tiling strategy is introduced to determine optimal SRAM block sizes.
    </p>
    <p>
        Compared with the standard attention, SWattention achieves around 2.0x speedup for FP32 training and 2.5x speedup for mixed-precision training. The sequence lengths range from 1k to 8k and scale up to 16k without being out of memory. As for the end-to-end performance, SWattention achieves up to 1.26$\times$ speedup for training GPT-style models, which demonstrates that SWattention enables longer sequence length for LLM training.
    </p>

    <p>
        <a href="http://zhuxy-USTC.github.io/files/swattention.pdf">Original paper</a> and <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2ajuuEAAAAJ&citation_for_view=k2ajuuEAAAAJ:qjMakFHDy7sC" target="_blank">Google Scholar Link</a>
    </p>

</body>
</html>
